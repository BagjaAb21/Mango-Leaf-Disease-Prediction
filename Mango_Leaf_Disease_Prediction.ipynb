{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Trhxo5KgVnN9",
        "outputId": "5e0cb61f-b7a5-4227-f6a0-223edb5fbfd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.15,>=2.14.0 (from tensorflow)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Installing collected packages: wrapt, tensorflow-estimator, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.15.0\n",
            "    Uninstalling wrapt-1.15.0:\n",
            "      Successfully uninstalled wrapt-1.15.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.13.0\n",
            "    Uninstalling tensorflow-estimator-2.13.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.13.1\n",
            "    Uninstalling keras-2.13.1:\n",
            "      Successfully uninstalled keras-2.13.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.13.0\n",
            "    Uninstalling tensorboard-2.13.0:\n",
            "      Successfully uninstalled tensorboard-2.13.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.13.0\n",
            "    Uninstalling tensorflow-2.13.0:\n",
            "      Successfully uninstalled tensorflow-2.13.0\n",
            "Successfully installed keras-2.14.0 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 wrapt-1.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install requests\n",
        "!pip install Pillow\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnlJ4gPGWEgI",
        "outputId": "fb4e410b-6e51-4ec7-ea9a-e16bed4fff11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Necessary Libraries"
      ],
      "metadata": {
        "id": "Wkn6WK3GXWzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "tUkm-DH-XTPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Load and Organize the Dataset"
      ],
      "metadata": {
        "id": "b5enFYkSXse5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# Define the URL of the dataset zip file\n",
        "dataset_url = \"https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/hxsnvwty3r-1.zip\"\n",
        "\n",
        "# Define the destination directory where you want to save and extract the dataset\n",
        "destination_dir = \"/content/MangoLeafBD_Dataset\"\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "!mkdir -p {destination_dir}\n",
        "\n",
        "# Download the dataset zip file\n",
        "response = requests.get(dataset_url)\n",
        "dataset_zip = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "\n",
        "# Extract the dataset to the destination directory\n",
        "dataset_zip.extractall(destination_dir)\n",
        "\n",
        "# Close the zip file\n",
        "dataset_zip.close()\n",
        "\n",
        "print(\"Dataset downloaded and extracted successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z0hz2ZRYANe",
        "outputId": "3088da30-768e-4c58-ec9f-efd5ba1a50db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded and extracted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the root dataset directory\n",
        "root_dir = \"/content/MangoLeafBD_Dataset/MangoLeafBD Dataset\"\n",
        "\n",
        "# Define the target train and validation directories\n",
        "train_dir = \"/content/MangoLeafBD_Train\"\n",
        "val_dir = \"/content/MangoLeafBD_Val\"\n",
        "\n",
        "# Create train and validation directories\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "# Define the class names\n",
        "class_names = [\n",
        "    \"Anthracnose\", \"Bacterial Canker\", \"Cutting Weevil\", \"Die Back\",\n",
        "    \"Gall Midge\", \"Healthy\", \"Powdery Mildew\", \"Sooty Mould\"\n",
        "]\n",
        "\n",
        "# Define the train-validation split ratio (adjust as needed)\n",
        "val_split = 0.2\n",
        "\n",
        "# Iterate through class names and move images to train and validation folders\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(root_dir, class_name)\n",
        "    train_class_dir = os.path.join(train_dir, class_name)\n",
        "    val_class_dir = os.path.join(val_dir, class_name)\n",
        "\n",
        "    # Create class directories in train and validation folders\n",
        "    os.makedirs(train_class_dir, exist_ok=True)\n",
        "    os.makedirs(val_class_dir, exist_ok=True)\n",
        "\n",
        "    # List all images in the class directory\n",
        "    images = os.listdir(class_dir)\n",
        "\n",
        "    # Split images into train and validation sets\n",
        "    train_images, val_images = train_test_split(images, test_size=val_split, random_state=42)\n",
        "\n",
        "    # Move images to train and validation folders\n",
        "    for image in train_images:\n",
        "        src = os.path.join(class_dir, image)\n",
        "        dst = os.path.join(train_class_dir, image)\n",
        "        shutil.move(src, dst)\n",
        "\n",
        "    for image in val_images:\n",
        "        src = os.path.join(class_dir, image)\n",
        "        dst = os.path.join(val_class_dir, image)\n",
        "        shutil.move(src, dst)\n",
        "\n",
        "print(\"Dataset organized into train and validation folders.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c0J980PXdnX",
        "outputId": "61be7368-1492-4a5d-ce79-eb2179976ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset organized into train and validation folders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Data Augmentation"
      ],
      "metadata": {
        "id": "ewO25HopYHnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image preprocessing parameters\n",
        "batch_size = 32\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Create data generators with data augmentation for training and validation data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnnWISQtYDqZ",
        "outputId": "36b97c4f-9e1a-40c8-bccc-7eb7a3b1f4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3200 images belonging to 8 classes.\n",
            "Found 800 images belonging to 8 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Build a Complex CNN Model"
      ],
      "metadata": {
        "id": "MyesSR6DYPqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    # Convolutional layers\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Flatten the output and add fully connected layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(8, activation='softmax')  # 8 classes for your dataset\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "a-ymZ7t1YMMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Train the Model"
      ],
      "metadata": {
        "id": "izRu5imRYUAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of epochs\n",
        "epochs = 20\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    validation_steps=len(val_generator)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ykr7MXrYSWU",
        "outputId": "aa913c62-a789-41c2-a0dd-30c402190515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "100/100 [==============================] - 63s 480ms/step - loss: 1.8318 - accuracy: 0.2559 - val_loss: 1.4914 - val_accuracy: 0.4238\n",
            "Epoch 2/20\n",
            "100/100 [==============================] - 50s 501ms/step - loss: 1.2359 - accuracy: 0.5288 - val_loss: 0.7422 - val_accuracy: 0.7300\n",
            "Epoch 3/20\n",
            "100/100 [==============================] - 53s 529ms/step - loss: 0.8498 - accuracy: 0.6891 - val_loss: 0.5606 - val_accuracy: 0.7713\n",
            "Epoch 4/20\n",
            "100/100 [==============================] - 49s 495ms/step - loss: 0.5974 - accuracy: 0.7925 - val_loss: 0.3965 - val_accuracy: 0.8700\n",
            "Epoch 5/20\n",
            "100/100 [==============================] - 51s 506ms/step - loss: 0.5244 - accuracy: 0.8106 - val_loss: 0.2462 - val_accuracy: 0.9237\n",
            "Epoch 6/20\n",
            "100/100 [==============================] - 48s 484ms/step - loss: 0.3586 - accuracy: 0.8703 - val_loss: 0.4718 - val_accuracy: 0.8288\n",
            "Epoch 7/20\n",
            "100/100 [==============================] - 48s 477ms/step - loss: 0.3859 - accuracy: 0.8637 - val_loss: 0.2203 - val_accuracy: 0.9438\n",
            "Epoch 8/20\n",
            "100/100 [==============================] - 48s 480ms/step - loss: 0.2686 - accuracy: 0.9087 - val_loss: 0.3867 - val_accuracy: 0.8650\n",
            "Epoch 9/20\n",
            "100/100 [==============================] - 48s 477ms/step - loss: 0.2752 - accuracy: 0.9044 - val_loss: 0.2541 - val_accuracy: 0.9013\n",
            "Epoch 10/20\n",
            "100/100 [==============================] - 47s 474ms/step - loss: 0.2310 - accuracy: 0.9153 - val_loss: 0.2350 - val_accuracy: 0.9175\n",
            "Epoch 11/20\n",
            "100/100 [==============================] - 45s 452ms/step - loss: 0.2172 - accuracy: 0.9253 - val_loss: 0.2329 - val_accuracy: 0.9212\n",
            "Epoch 12/20\n",
            "100/100 [==============================] - 46s 461ms/step - loss: 0.2397 - accuracy: 0.9187 - val_loss: 0.0951 - val_accuracy: 0.9663\n",
            "Epoch 13/20\n",
            "100/100 [==============================] - 45s 447ms/step - loss: 0.1715 - accuracy: 0.9359 - val_loss: 0.0911 - val_accuracy: 0.9675\n",
            "Epoch 14/20\n",
            "100/100 [==============================] - 50s 501ms/step - loss: 0.2504 - accuracy: 0.9187 - val_loss: 0.1014 - val_accuracy: 0.9725\n",
            "Epoch 15/20\n",
            "100/100 [==============================] - 47s 466ms/step - loss: 0.1488 - accuracy: 0.9494 - val_loss: 0.1403 - val_accuracy: 0.9563\n",
            "Epoch 16/20\n",
            "100/100 [==============================] - 48s 480ms/step - loss: 0.2002 - accuracy: 0.9356 - val_loss: 0.2020 - val_accuracy: 0.9350\n",
            "Epoch 17/20\n",
            "100/100 [==============================] - 48s 479ms/step - loss: 0.2147 - accuracy: 0.9259 - val_loss: 0.1900 - val_accuracy: 0.9388\n",
            "Epoch 18/20\n",
            "100/100 [==============================] - 46s 466ms/step - loss: 0.1709 - accuracy: 0.9441 - val_loss: 0.1614 - val_accuracy: 0.9538\n",
            "Epoch 19/20\n",
            "100/100 [==============================] - 47s 474ms/step - loss: 0.1621 - accuracy: 0.9481 - val_loss: 0.0678 - val_accuracy: 0.9825\n",
            "Epoch 20/20\n",
            "100/100 [==============================] - 45s 453ms/step - loss: 0.1464 - accuracy: 0.9491 - val_loss: 0.0587 - val_accuracy: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Evaluate the Model"
      ],
      "metadata": {
        "id": "_Nx1h8RUYYcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the training data\n",
        "train_metrics = model.evaluate(train_generator)\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "val_metrics = model.evaluate(val_generator)\n",
        "\n",
        "# Print the metrics for the training set\n",
        "print(\"Metrics for Training Set:\")\n",
        "print(f\"Loss: {train_metrics[0]}\")\n",
        "print(f\"Accuracy: {train_metrics[1]}\")\n",
        "\n",
        "# Print the metrics for the validation set\n",
        "print(\"\\nMetrics for Validation Set:\")\n",
        "print(f\"Loss: {val_metrics[0]}\")\n",
        "print(f\"Accuracy: {val_metrics[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXbBVbC3YWnU",
        "outputId": "9867aa02-ebf4-4c04-f6e6-257be7f65a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 45s 449ms/step - loss: 0.0668 - accuracy: 0.9744\n",
            "25/25 [==============================] - 1s 54ms/step - loss: 0.0587 - accuracy: 0.9825\n",
            "Metrics for Training Set:\n",
            "Loss: 0.06675581634044647\n",
            "Accuracy: 0.9743750095367432\n",
            "\n",
            "Metrics for Validation Set:\n",
            "Loss: 0.05869104340672493\n",
            "Accuracy: 0.9825000166893005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Save the Model"
      ],
      "metadata": {
        "id": "nDnqyWZ2YcRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model.save(\"mango_leaf_disease_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbPY2bfPYanF",
        "outputId": "521167fc-3f96-45e9-c0e7-36eea828eb98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Make Predictions"
      ],
      "metadata": {
        "id": "JrWIZG9kYrus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.1: Load the Trained Model\n",
        "\n",
        "  First, load the saved model you trained earlier. Make sure you have the correct path to the saved model (e.g., \"mango_leaf_disease_model.h5\")."
      ],
      "metadata": {
        "id": "kY8cZNPoYy_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "loaded_model = keras.models.load_model(\"mango_leaf_disease_model.h5\")"
      ],
      "metadata": {
        "id": "owoFSVG4YefT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.2: Preprocess New Images\n",
        "\n",
        "Before making predictions, you need to preprocess the new images in the same way you preprocessed the training and validation data."
      ],
      "metadata": {
        "id": "d_DWLEjPZPSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Define the path to the new image you want to test\n",
        "new_image_path = \"/path/to/your/new/image.jpg\"\n",
        "\n",
        "# Load and preprocess the new image\n",
        "img = image.load_img(new_image_path, target_size=image_size)\n",
        "img = image.img_to_array(img)\n",
        "img = np.expand_dims(img, axis=0)\n",
        "img = img / 255.0  # Normalize pixel values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "jX6WbBKvZayI",
        "outputId": "c20f7735-1f52-4675-ddaa-08d68f9321ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d2e33244e237>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load and preprocess the new image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_image_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/your/new/image.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.3: Make Predictions\n",
        "\n",
        "Now, you can use the loaded model to make predictions on the preprocessed image:"
      ],
      "metadata": {
        "id": "NrM021Q8ZeQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = loaded_model.predict(img)\n",
        "\n",
        "# Get class labels and corresponding probabilities\n",
        "class_labels = train_generator.class_indices\n",
        "class_labels = {v: k for k, v in class_labels.items()}\n",
        "predicted_class = class_labels[np.argmax(predictions)]\n",
        "\n",
        "# Print the result\n",
        "print(f\"The predicted class is: {predicted_class}\")\n",
        "print(f\"Prediction probabilities: {predictions[0]}\")"
      ],
      "metadata": {
        "id": "9XQKguszZrGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Save the Model to Google Drive\n",
        "To save the trained model to Google Drive, follow these steps:"
      ],
      "metadata": {
        "id": "V2OrUVyQYsJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.1: Specify Source and Destination Paths\n",
        "\n",
        "Specify the source path, which is the path to your saved model, and the destination path in your Google Drive where you want to save the model."
      ],
      "metadata": {
        "id": "UTJEivKqZwbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_path = 'mango_leaf_disease_model.h5'  # Your model file\n",
        "destination_path = '/content/drive/MyDrive/'  # Destination in your Google Drive"
      ],
      "metadata": {
        "id": "eLiMChAZZorp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.2: Copy the Model to Google Drive\n",
        "\n",
        "\n",
        "Use the shutil library to copy the model from your Colab environment to Google Drive:"
      ],
      "metadata": {
        "id": "rKalIuCQeOow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Copy the model to Google Drive\n",
        "shutil.copy(source_path, destination_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lmHXuuI9Z0bY",
        "outputId": "754289e5-cff4-4a4c-cf50-ef48a33db051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/mango_leaf_disease_model.h5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sbWhlglVeRIA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}